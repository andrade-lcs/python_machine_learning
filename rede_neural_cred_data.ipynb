{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rede_neural_cred_data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNr3N5xmD7D28AUd+mHsGZ1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTqXDEEWc9Hl"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xehNSauEdueh"
      },
      "source": [
        "Importação de Banco de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uueSwgrxdf2Z"
      },
      "source": [
        "base = pd.read_csv('credit_data.csv')\n",
        "base.loc[base.age < 0, 'age'] = 40.92\n",
        "               \n",
        "previsores = base.iloc[:, 1:4].values\n",
        "classe = base.iloc[:, 4].values"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfegwg_EdzbL"
      },
      "source": [
        "Tratando valores faltantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dirfZKGtdjaq"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
        "imputer = imputer.fit(previsores[:, 1:4])\n",
        "previsores[:, 1:4] = imputer.transform(previsores[:, 1:4])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2hTsgfAd3rx"
      },
      "source": [
        "Padronizando os valores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6WQBzXPdlYC"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0fuikdHeHNX"
      },
      "source": [
        "Teste do banco de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfv1_VJFdn8S"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25, random_state=0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ExmLlAuO8y3"
      },
      "source": [
        "Treinamento Rede Neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAp_G34xdqGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0552f9c3-5513-4ad6-b729-41d3dbb44c7c"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\r\n",
        "classificador = MLPClassifier(verbose=True,\r\n",
        "                              max_iter=1000,\r\n",
        "                              tol=0.000010,\r\n",
        "                              solver='adam',\r\n",
        "                              hidden_layer_sizes=(100),\r\n",
        "                              activation='relu')\r\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.63340239\n",
            "Iteration 2, loss = 0.57809562\n",
            "Iteration 3, loss = 0.52993868\n",
            "Iteration 4, loss = 0.48812155\n",
            "Iteration 5, loss = 0.45027616\n",
            "Iteration 6, loss = 0.41723438\n",
            "Iteration 7, loss = 0.38687819\n",
            "Iteration 8, loss = 0.35981938\n",
            "Iteration 9, loss = 0.33501168\n",
            "Iteration 10, loss = 0.31249227\n",
            "Iteration 11, loss = 0.29220740\n",
            "Iteration 12, loss = 0.27368454\n",
            "Iteration 13, loss = 0.25700069\n",
            "Iteration 14, loss = 0.24207261\n",
            "Iteration 15, loss = 0.22844128\n",
            "Iteration 16, loss = 0.21636946\n",
            "Iteration 17, loss = 0.20534411\n",
            "Iteration 18, loss = 0.19532794\n",
            "Iteration 19, loss = 0.18658150\n",
            "Iteration 20, loss = 0.17838156\n",
            "Iteration 21, loss = 0.17101946\n",
            "Iteration 22, loss = 0.16422960\n",
            "Iteration 23, loss = 0.15820826\n",
            "Iteration 24, loss = 0.15253220\n",
            "Iteration 25, loss = 0.14744636\n",
            "Iteration 26, loss = 0.14271812\n",
            "Iteration 27, loss = 0.13827016\n",
            "Iteration 28, loss = 0.13422797\n",
            "Iteration 29, loss = 0.13051386\n",
            "Iteration 30, loss = 0.12698864\n",
            "Iteration 31, loss = 0.12376621\n",
            "Iteration 32, loss = 0.12069504\n",
            "Iteration 33, loss = 0.11781946\n",
            "Iteration 34, loss = 0.11510992\n",
            "Iteration 35, loss = 0.11261051\n",
            "Iteration 36, loss = 0.11019501\n",
            "Iteration 37, loss = 0.10795705\n",
            "Iteration 38, loss = 0.10579435\n",
            "Iteration 39, loss = 0.10368678\n",
            "Iteration 40, loss = 0.10175566\n",
            "Iteration 41, loss = 0.09995271\n",
            "Iteration 42, loss = 0.09819826\n",
            "Iteration 43, loss = 0.09650877\n",
            "Iteration 44, loss = 0.09500688\n",
            "Iteration 45, loss = 0.09339644\n",
            "Iteration 46, loss = 0.09200819\n",
            "Iteration 47, loss = 0.09069560\n",
            "Iteration 48, loss = 0.08933395\n",
            "Iteration 49, loss = 0.08799845\n",
            "Iteration 50, loss = 0.08681062\n",
            "Iteration 51, loss = 0.08563651\n",
            "Iteration 52, loss = 0.08449368\n",
            "Iteration 53, loss = 0.08348896\n",
            "Iteration 54, loss = 0.08230081\n",
            "Iteration 55, loss = 0.08125963\n",
            "Iteration 56, loss = 0.08029295\n",
            "Iteration 57, loss = 0.07949448\n",
            "Iteration 58, loss = 0.07865412\n",
            "Iteration 59, loss = 0.07743806\n",
            "Iteration 60, loss = 0.07662209\n",
            "Iteration 61, loss = 0.07581772\n",
            "Iteration 62, loss = 0.07485798\n",
            "Iteration 63, loss = 0.07411369\n",
            "Iteration 64, loss = 0.07332477\n",
            "Iteration 65, loss = 0.07252624\n",
            "Iteration 66, loss = 0.07168058\n",
            "Iteration 67, loss = 0.07094456\n",
            "Iteration 68, loss = 0.07023397\n",
            "Iteration 69, loss = 0.06955372\n",
            "Iteration 70, loss = 0.06883709\n",
            "Iteration 71, loss = 0.06812487\n",
            "Iteration 72, loss = 0.06745011\n",
            "Iteration 73, loss = 0.06691573\n",
            "Iteration 74, loss = 0.06609640\n",
            "Iteration 75, loss = 0.06548767\n",
            "Iteration 76, loss = 0.06483584\n",
            "Iteration 77, loss = 0.06424917\n",
            "Iteration 78, loss = 0.06363521\n",
            "Iteration 79, loss = 0.06311053\n",
            "Iteration 80, loss = 0.06254043\n",
            "Iteration 81, loss = 0.06182317\n",
            "Iteration 82, loss = 0.06140609\n",
            "Iteration 83, loss = 0.06066789\n",
            "Iteration 84, loss = 0.06005343\n",
            "Iteration 85, loss = 0.05952413\n",
            "Iteration 86, loss = 0.05894953\n",
            "Iteration 87, loss = 0.05846268\n",
            "Iteration 88, loss = 0.05790308\n",
            "Iteration 89, loss = 0.05739116\n",
            "Iteration 90, loss = 0.05688653\n",
            "Iteration 91, loss = 0.05637308\n",
            "Iteration 92, loss = 0.05584548\n",
            "Iteration 93, loss = 0.05539591\n",
            "Iteration 94, loss = 0.05490981\n",
            "Iteration 95, loss = 0.05444096\n",
            "Iteration 96, loss = 0.05396754\n",
            "Iteration 97, loss = 0.05355135\n",
            "Iteration 98, loss = 0.05306273\n",
            "Iteration 99, loss = 0.05265857\n",
            "Iteration 100, loss = 0.05217092\n",
            "Iteration 101, loss = 0.05178676\n",
            "Iteration 102, loss = 0.05143688\n",
            "Iteration 103, loss = 0.05091277\n",
            "Iteration 104, loss = 0.05054735\n",
            "Iteration 105, loss = 0.05002242\n",
            "Iteration 106, loss = 0.04966607\n",
            "Iteration 107, loss = 0.04938688\n",
            "Iteration 108, loss = 0.04893527\n",
            "Iteration 109, loss = 0.04854146\n",
            "Iteration 110, loss = 0.04804880\n",
            "Iteration 111, loss = 0.04768597\n",
            "Iteration 112, loss = 0.04732478\n",
            "Iteration 113, loss = 0.04696295\n",
            "Iteration 114, loss = 0.04659580\n",
            "Iteration 115, loss = 0.04618938\n",
            "Iteration 116, loss = 0.04584812\n",
            "Iteration 117, loss = 0.04546327\n",
            "Iteration 118, loss = 0.04515517\n",
            "Iteration 119, loss = 0.04477824\n",
            "Iteration 120, loss = 0.04458359\n",
            "Iteration 121, loss = 0.04405798\n",
            "Iteration 122, loss = 0.04403541\n",
            "Iteration 123, loss = 0.04350417\n",
            "Iteration 124, loss = 0.04315777\n",
            "Iteration 125, loss = 0.04285776\n",
            "Iteration 126, loss = 0.04256797\n",
            "Iteration 127, loss = 0.04220813\n",
            "Iteration 128, loss = 0.04192057\n",
            "Iteration 129, loss = 0.04165415\n",
            "Iteration 130, loss = 0.04134185\n",
            "Iteration 131, loss = 0.04105252\n",
            "Iteration 132, loss = 0.04081011\n",
            "Iteration 133, loss = 0.04052563\n",
            "Iteration 134, loss = 0.04019639\n",
            "Iteration 135, loss = 0.04003243\n",
            "Iteration 136, loss = 0.03978895\n",
            "Iteration 137, loss = 0.03940989\n",
            "Iteration 138, loss = 0.03922348\n",
            "Iteration 139, loss = 0.03892209\n",
            "Iteration 140, loss = 0.03873991\n",
            "Iteration 141, loss = 0.03851771\n",
            "Iteration 142, loss = 0.03815274\n",
            "Iteration 143, loss = 0.03806011\n",
            "Iteration 144, loss = 0.03780366\n",
            "Iteration 145, loss = 0.03750614\n",
            "Iteration 146, loss = 0.03734406\n",
            "Iteration 147, loss = 0.03730561\n",
            "Iteration 148, loss = 0.03687230\n",
            "Iteration 149, loss = 0.03660039\n",
            "Iteration 150, loss = 0.03636152\n",
            "Iteration 151, loss = 0.03616009\n",
            "Iteration 152, loss = 0.03596188\n",
            "Iteration 153, loss = 0.03573917\n",
            "Iteration 154, loss = 0.03552374\n",
            "Iteration 155, loss = 0.03534064\n",
            "Iteration 156, loss = 0.03514103\n",
            "Iteration 157, loss = 0.03494812\n",
            "Iteration 158, loss = 0.03473968\n",
            "Iteration 159, loss = 0.03460677\n",
            "Iteration 160, loss = 0.03438338\n",
            "Iteration 161, loss = 0.03419809\n",
            "Iteration 162, loss = 0.03398525\n",
            "Iteration 163, loss = 0.03376983\n",
            "Iteration 164, loss = 0.03362555\n",
            "Iteration 165, loss = 0.03339708\n",
            "Iteration 166, loss = 0.03335304\n",
            "Iteration 167, loss = 0.03313822\n",
            "Iteration 168, loss = 0.03284488\n",
            "Iteration 169, loss = 0.03272414\n",
            "Iteration 170, loss = 0.03256309\n",
            "Iteration 171, loss = 0.03238988\n",
            "Iteration 172, loss = 0.03215977\n",
            "Iteration 173, loss = 0.03197705\n",
            "Iteration 174, loss = 0.03185350\n",
            "Iteration 175, loss = 0.03169975\n",
            "Iteration 176, loss = 0.03151853\n",
            "Iteration 177, loss = 0.03131432\n",
            "Iteration 178, loss = 0.03116197\n",
            "Iteration 179, loss = 0.03109038\n",
            "Iteration 180, loss = 0.03092742\n",
            "Iteration 181, loss = 0.03071055\n",
            "Iteration 182, loss = 0.03070696\n",
            "Iteration 183, loss = 0.03044275\n",
            "Iteration 184, loss = 0.03029861\n",
            "Iteration 185, loss = 0.03014624\n",
            "Iteration 186, loss = 0.02998640\n",
            "Iteration 187, loss = 0.02988305\n",
            "Iteration 188, loss = 0.02973769\n",
            "Iteration 189, loss = 0.02960929\n",
            "Iteration 190, loss = 0.02947634\n",
            "Iteration 191, loss = 0.02929296\n",
            "Iteration 192, loss = 0.02914178\n",
            "Iteration 193, loss = 0.02900526\n",
            "Iteration 194, loss = 0.02887458\n",
            "Iteration 195, loss = 0.02876428\n",
            "Iteration 196, loss = 0.02861235\n",
            "Iteration 197, loss = 0.02846133\n",
            "Iteration 198, loss = 0.02833022\n",
            "Iteration 199, loss = 0.02819075\n",
            "Iteration 200, loss = 0.02807054\n",
            "Iteration 201, loss = 0.02799158\n",
            "Iteration 202, loss = 0.02785246\n",
            "Iteration 203, loss = 0.02774731\n",
            "Iteration 204, loss = 0.02762585\n",
            "Iteration 205, loss = 0.02747169\n",
            "Iteration 206, loss = 0.02732013\n",
            "Iteration 207, loss = 0.02724198\n",
            "Iteration 208, loss = 0.02707024\n",
            "Iteration 209, loss = 0.02697170\n",
            "Iteration 210, loss = 0.02683868\n",
            "Iteration 211, loss = 0.02670379\n",
            "Iteration 212, loss = 0.02670430\n",
            "Iteration 213, loss = 0.02656783\n",
            "Iteration 214, loss = 0.02637247\n",
            "Iteration 215, loss = 0.02630990\n",
            "Iteration 216, loss = 0.02611987\n",
            "Iteration 217, loss = 0.02609015\n",
            "Iteration 218, loss = 0.02596337\n",
            "Iteration 219, loss = 0.02589900\n",
            "Iteration 220, loss = 0.02570698\n",
            "Iteration 221, loss = 0.02560384\n",
            "Iteration 222, loss = 0.02550371\n",
            "Iteration 223, loss = 0.02537785\n",
            "Iteration 224, loss = 0.02539001\n",
            "Iteration 225, loss = 0.02520973\n",
            "Iteration 226, loss = 0.02508521\n",
            "Iteration 227, loss = 0.02496686\n",
            "Iteration 228, loss = 0.02484402\n",
            "Iteration 229, loss = 0.02475689\n",
            "Iteration 230, loss = 0.02467257\n",
            "Iteration 231, loss = 0.02457308\n",
            "Iteration 232, loss = 0.02455805\n",
            "Iteration 233, loss = 0.02439329\n",
            "Iteration 234, loss = 0.02428943\n",
            "Iteration 235, loss = 0.02418408\n",
            "Iteration 236, loss = 0.02412731\n",
            "Iteration 237, loss = 0.02399268\n",
            "Iteration 238, loss = 0.02389050\n",
            "Iteration 239, loss = 0.02381058\n",
            "Iteration 240, loss = 0.02370755\n",
            "Iteration 241, loss = 0.02359041\n",
            "Iteration 242, loss = 0.02357822\n",
            "Iteration 243, loss = 0.02343487\n",
            "Iteration 244, loss = 0.02332123\n",
            "Iteration 245, loss = 0.02329673\n",
            "Iteration 246, loss = 0.02315821\n",
            "Iteration 247, loss = 0.02320898\n",
            "Iteration 248, loss = 0.02294761\n",
            "Iteration 249, loss = 0.02288994\n",
            "Iteration 250, loss = 0.02279357\n",
            "Iteration 251, loss = 0.02276314\n",
            "Iteration 252, loss = 0.02267899\n",
            "Iteration 253, loss = 0.02252308\n",
            "Iteration 254, loss = 0.02244541\n",
            "Iteration 255, loss = 0.02247008\n",
            "Iteration 256, loss = 0.02225880\n",
            "Iteration 257, loss = 0.02217809\n",
            "Iteration 258, loss = 0.02211786\n",
            "Iteration 259, loss = 0.02204566\n",
            "Iteration 260, loss = 0.02196171\n",
            "Iteration 261, loss = 0.02194119\n",
            "Iteration 262, loss = 0.02175824\n",
            "Iteration 263, loss = 0.02171198\n",
            "Iteration 264, loss = 0.02168332\n",
            "Iteration 265, loss = 0.02151422\n",
            "Iteration 266, loss = 0.02151369\n",
            "Iteration 267, loss = 0.02146558\n",
            "Iteration 268, loss = 0.02138082\n",
            "Iteration 269, loss = 0.02125069\n",
            "Iteration 270, loss = 0.02125583\n",
            "Iteration 271, loss = 0.02102098\n",
            "Iteration 272, loss = 0.02106265\n",
            "Iteration 273, loss = 0.02093709\n",
            "Iteration 274, loss = 0.02086776\n",
            "Iteration 275, loss = 0.02074453\n",
            "Iteration 276, loss = 0.02076980\n",
            "Iteration 277, loss = 0.02059745\n",
            "Iteration 278, loss = 0.02054266\n",
            "Iteration 279, loss = 0.02044826\n",
            "Iteration 280, loss = 0.02045700\n",
            "Iteration 281, loss = 0.02030893\n",
            "Iteration 282, loss = 0.02031449\n",
            "Iteration 283, loss = 0.02021363\n",
            "Iteration 284, loss = 0.02014124\n",
            "Iteration 285, loss = 0.02008062\n",
            "Iteration 286, loss = 0.02004407\n",
            "Iteration 287, loss = 0.01999892\n",
            "Iteration 288, loss = 0.01990759\n",
            "Iteration 289, loss = 0.01973566\n",
            "Iteration 290, loss = 0.01982127\n",
            "Iteration 291, loss = 0.01965722\n",
            "Iteration 292, loss = 0.01960684\n",
            "Iteration 293, loss = 0.01960034\n",
            "Iteration 294, loss = 0.01944205\n",
            "Iteration 295, loss = 0.01937217\n",
            "Iteration 296, loss = 0.01931275\n",
            "Iteration 297, loss = 0.01922911\n",
            "Iteration 298, loss = 0.01920777\n",
            "Iteration 299, loss = 0.01911633\n",
            "Iteration 300, loss = 0.01905798\n",
            "Iteration 301, loss = 0.01903930\n",
            "Iteration 302, loss = 0.01900829\n",
            "Iteration 303, loss = 0.01886171\n",
            "Iteration 304, loss = 0.01881918\n",
            "Iteration 305, loss = 0.01876790\n",
            "Iteration 306, loss = 0.01872576\n",
            "Iteration 307, loss = 0.01863153\n",
            "Iteration 308, loss = 0.01870751\n",
            "Iteration 309, loss = 0.01861113\n",
            "Iteration 310, loss = 0.01844714\n",
            "Iteration 311, loss = 0.01848019\n",
            "Iteration 312, loss = 0.01844004\n",
            "Iteration 313, loss = 0.01829938\n",
            "Iteration 314, loss = 0.01823208\n",
            "Iteration 315, loss = 0.01818829\n",
            "Iteration 316, loss = 0.01813296\n",
            "Iteration 317, loss = 0.01806208\n",
            "Iteration 318, loss = 0.01804742\n",
            "Iteration 319, loss = 0.01800046\n",
            "Iteration 320, loss = 0.01799998\n",
            "Iteration 321, loss = 0.01788675\n",
            "Iteration 322, loss = 0.01788081\n",
            "Iteration 323, loss = 0.01775433\n",
            "Iteration 324, loss = 0.01768940\n",
            "Iteration 325, loss = 0.01769885\n",
            "Iteration 326, loss = 0.01760926\n",
            "Iteration 327, loss = 0.01752278\n",
            "Iteration 328, loss = 0.01752170\n",
            "Iteration 329, loss = 0.01734495\n",
            "Iteration 330, loss = 0.01738692\n",
            "Iteration 331, loss = 0.01733094\n",
            "Iteration 332, loss = 0.01727439\n",
            "Iteration 333, loss = 0.01728834\n",
            "Iteration 334, loss = 0.01718521\n",
            "Iteration 335, loss = 0.01713046\n",
            "Iteration 336, loss = 0.01699508\n",
            "Iteration 337, loss = 0.01695774\n",
            "Iteration 338, loss = 0.01695388\n",
            "Iteration 339, loss = 0.01689250\n",
            "Iteration 340, loss = 0.01685091\n",
            "Iteration 341, loss = 0.01683141\n",
            "Iteration 342, loss = 0.01671491\n",
            "Iteration 343, loss = 0.01669201\n",
            "Iteration 344, loss = 0.01669300\n",
            "Iteration 345, loss = 0.01662427\n",
            "Iteration 346, loss = 0.01656815\n",
            "Iteration 347, loss = 0.01649464\n",
            "Iteration 348, loss = 0.01646725\n",
            "Iteration 349, loss = 0.01651053\n",
            "Iteration 350, loss = 0.01634775\n",
            "Iteration 351, loss = 0.01627846\n",
            "Iteration 352, loss = 0.01634114\n",
            "Iteration 353, loss = 0.01623641\n",
            "Iteration 354, loss = 0.01621184\n",
            "Iteration 355, loss = 0.01611483\n",
            "Iteration 356, loss = 0.01608814\n",
            "Iteration 357, loss = 0.01605670\n",
            "Iteration 358, loss = 0.01598400\n",
            "Iteration 359, loss = 0.01595622\n",
            "Iteration 360, loss = 0.01593633\n",
            "Iteration 361, loss = 0.01590612\n",
            "Iteration 362, loss = 0.01585076\n",
            "Iteration 363, loss = 0.01580901\n",
            "Iteration 364, loss = 0.01575783\n",
            "Iteration 365, loss = 0.01571310\n",
            "Iteration 366, loss = 0.01567917\n",
            "Iteration 367, loss = 0.01562751\n",
            "Iteration 368, loss = 0.01552567\n",
            "Iteration 369, loss = 0.01547691\n",
            "Iteration 370, loss = 0.01544900\n",
            "Iteration 371, loss = 0.01538655\n",
            "Iteration 372, loss = 0.01536314\n",
            "Iteration 373, loss = 0.01529233\n",
            "Iteration 374, loss = 0.01531341\n",
            "Iteration 375, loss = 0.01523532\n",
            "Iteration 376, loss = 0.01517324\n",
            "Iteration 377, loss = 0.01513923\n",
            "Iteration 378, loss = 0.01514252\n",
            "Iteration 379, loss = 0.01509185\n",
            "Iteration 380, loss = 0.01501451\n",
            "Iteration 381, loss = 0.01509636\n",
            "Iteration 382, loss = 0.01502697\n",
            "Iteration 383, loss = 0.01493690\n",
            "Iteration 384, loss = 0.01488549\n",
            "Iteration 385, loss = 0.01481404\n",
            "Iteration 386, loss = 0.01478910\n",
            "Iteration 387, loss = 0.01490015\n",
            "Iteration 388, loss = 0.01484384\n",
            "Iteration 389, loss = 0.01475313\n",
            "Iteration 390, loss = 0.01461809\n",
            "Iteration 391, loss = 0.01462264\n",
            "Iteration 392, loss = 0.01455163\n",
            "Iteration 393, loss = 0.01454847\n",
            "Iteration 394, loss = 0.01453173\n",
            "Iteration 395, loss = 0.01449012\n",
            "Iteration 396, loss = 0.01452712\n",
            "Iteration 397, loss = 0.01442147\n",
            "Iteration 398, loss = 0.01435280\n",
            "Iteration 399, loss = 0.01427968\n",
            "Iteration 400, loss = 0.01432039\n",
            "Iteration 401, loss = 0.01422285\n",
            "Iteration 402, loss = 0.01430907\n",
            "Iteration 403, loss = 0.01417496\n",
            "Iteration 404, loss = 0.01411753\n",
            "Iteration 405, loss = 0.01405762\n",
            "Iteration 406, loss = 0.01400378\n",
            "Iteration 407, loss = 0.01402414\n",
            "Iteration 408, loss = 0.01393983\n",
            "Iteration 409, loss = 0.01392290\n",
            "Iteration 410, loss = 0.01389537\n",
            "Iteration 411, loss = 0.01378145\n",
            "Iteration 412, loss = 0.01385558\n",
            "Iteration 413, loss = 0.01393865\n",
            "Iteration 414, loss = 0.01371096\n",
            "Iteration 415, loss = 0.01368234\n",
            "Iteration 416, loss = 0.01365398\n",
            "Iteration 417, loss = 0.01363516\n",
            "Iteration 418, loss = 0.01359685\n",
            "Iteration 419, loss = 0.01362300\n",
            "Iteration 420, loss = 0.01359133\n",
            "Iteration 421, loss = 0.01349769\n",
            "Iteration 422, loss = 0.01352955\n",
            "Iteration 423, loss = 0.01342341\n",
            "Iteration 424, loss = 0.01342059\n",
            "Iteration 425, loss = 0.01337053\n",
            "Iteration 426, loss = 0.01330335\n",
            "Iteration 427, loss = 0.01334388\n",
            "Iteration 428, loss = 0.01325420\n",
            "Iteration 429, loss = 0.01331691\n",
            "Iteration 430, loss = 0.01317530\n",
            "Iteration 431, loss = 0.01312612\n",
            "Iteration 432, loss = 0.01316375\n",
            "Iteration 433, loss = 0.01306181\n",
            "Iteration 434, loss = 0.01308624\n",
            "Iteration 435, loss = 0.01301950\n",
            "Iteration 436, loss = 0.01297457\n",
            "Iteration 437, loss = 0.01307462\n",
            "Iteration 438, loss = 0.01291159\n",
            "Iteration 439, loss = 0.01291140\n",
            "Iteration 440, loss = 0.01286937\n",
            "Iteration 441, loss = 0.01283330\n",
            "Iteration 442, loss = 0.01281108\n",
            "Iteration 443, loss = 0.01283864\n",
            "Iteration 444, loss = 0.01277355\n",
            "Iteration 445, loss = 0.01276192\n",
            "Iteration 446, loss = 0.01267315\n",
            "Iteration 447, loss = 0.01262085\n",
            "Iteration 448, loss = 0.01258724\n",
            "Iteration 449, loss = 0.01257414\n",
            "Iteration 450, loss = 0.01252273\n",
            "Iteration 451, loss = 0.01249589\n",
            "Iteration 452, loss = 0.01250250\n",
            "Iteration 453, loss = 0.01245903\n",
            "Iteration 454, loss = 0.01248877\n",
            "Iteration 455, loss = 0.01241426\n",
            "Iteration 456, loss = 0.01235521\n",
            "Iteration 457, loss = 0.01229732\n",
            "Iteration 458, loss = 0.01231240\n",
            "Iteration 459, loss = 0.01229898\n",
            "Iteration 460, loss = 0.01229054\n",
            "Iteration 461, loss = 0.01228642\n",
            "Iteration 462, loss = 0.01218221\n",
            "Iteration 463, loss = 0.01216739\n",
            "Iteration 464, loss = 0.01226815\n",
            "Iteration 465, loss = 0.01205294\n",
            "Iteration 466, loss = 0.01213122\n",
            "Iteration 467, loss = 0.01207779\n",
            "Iteration 468, loss = 0.01207487\n",
            "Iteration 469, loss = 0.01202688\n",
            "Iteration 470, loss = 0.01194882\n",
            "Iteration 471, loss = 0.01193744\n",
            "Iteration 472, loss = 0.01191546\n",
            "Iteration 473, loss = 0.01188085\n",
            "Iteration 474, loss = 0.01185459\n",
            "Iteration 475, loss = 0.01192303\n",
            "Iteration 476, loss = 0.01175065\n",
            "Iteration 477, loss = 0.01174873\n",
            "Iteration 478, loss = 0.01175664\n",
            "Iteration 479, loss = 0.01176331\n",
            "Iteration 480, loss = 0.01177414\n",
            "Iteration 481, loss = 0.01165548\n",
            "Iteration 482, loss = 0.01168453\n",
            "Iteration 483, loss = 0.01162607\n",
            "Iteration 484, loss = 0.01155685\n",
            "Iteration 485, loss = 0.01154020\n",
            "Iteration 486, loss = 0.01155928\n",
            "Iteration 487, loss = 0.01157230\n",
            "Iteration 488, loss = 0.01145675\n",
            "Iteration 489, loss = 0.01145944\n",
            "Iteration 490, loss = 0.01145329\n",
            "Iteration 491, loss = 0.01145581\n",
            "Iteration 492, loss = 0.01137964\n",
            "Iteration 493, loss = 0.01138725\n",
            "Iteration 494, loss = 0.01134359\n",
            "Iteration 495, loss = 0.01139183\n",
            "Iteration 496, loss = 0.01133503\n",
            "Iteration 497, loss = 0.01128629\n",
            "Iteration 498, loss = 0.01121561\n",
            "Iteration 499, loss = 0.01121579\n",
            "Iteration 500, loss = 0.01115550\n",
            "Iteration 501, loss = 0.01114914\n",
            "Iteration 502, loss = 0.01112212\n",
            "Iteration 503, loss = 0.01108914\n",
            "Iteration 504, loss = 0.01106600\n",
            "Iteration 505, loss = 0.01109719\n",
            "Iteration 506, loss = 0.01102263\n",
            "Iteration 507, loss = 0.01110688\n",
            "Iteration 508, loss = 0.01113642\n",
            "Iteration 509, loss = 0.01096282\n",
            "Iteration 510, loss = 0.01092484\n",
            "Iteration 511, loss = 0.01090403\n",
            "Iteration 512, loss = 0.01088436\n",
            "Iteration 513, loss = 0.01083127\n",
            "Iteration 514, loss = 0.01082733\n",
            "Iteration 515, loss = 0.01082418\n",
            "Iteration 516, loss = 0.01084433\n",
            "Iteration 517, loss = 0.01072270\n",
            "Iteration 518, loss = 0.01072629\n",
            "Iteration 519, loss = 0.01073131\n",
            "Iteration 520, loss = 0.01074973\n",
            "Iteration 521, loss = 0.01072801\n",
            "Iteration 522, loss = 0.01064096\n",
            "Iteration 523, loss = 0.01062257\n",
            "Iteration 524, loss = 0.01063387\n",
            "Iteration 525, loss = 0.01056565\n",
            "Iteration 526, loss = 0.01057086\n",
            "Iteration 527, loss = 0.01051022\n",
            "Iteration 528, loss = 0.01047604\n",
            "Iteration 529, loss = 0.01045383\n",
            "Iteration 530, loss = 0.01048634\n",
            "Iteration 531, loss = 0.01049480\n",
            "Iteration 532, loss = 0.01044784\n",
            "Iteration 533, loss = 0.01033375\n",
            "Iteration 534, loss = 0.01036577\n",
            "Iteration 535, loss = 0.01035663\n",
            "Iteration 536, loss = 0.01034339\n",
            "Iteration 537, loss = 0.01036939\n",
            "Iteration 538, loss = 0.01029642\n",
            "Iteration 539, loss = 0.01022664\n",
            "Iteration 540, loss = 0.01022238\n",
            "Iteration 541, loss = 0.01023378\n",
            "Iteration 542, loss = 0.01025601\n",
            "Iteration 543, loss = 0.01016566\n",
            "Iteration 544, loss = 0.01012117\n",
            "Iteration 545, loss = 0.01021487\n",
            "Iteration 546, loss = 0.01012244\n",
            "Iteration 547, loss = 0.01009405\n",
            "Iteration 548, loss = 0.01008831\n",
            "Iteration 549, loss = 0.00999625\n",
            "Iteration 550, loss = 0.01005261\n",
            "Iteration 551, loss = 0.01000403\n",
            "Iteration 552, loss = 0.00999684\n",
            "Iteration 553, loss = 0.00994537\n",
            "Iteration 554, loss = 0.00992775\n",
            "Iteration 555, loss = 0.00991436\n",
            "Iteration 556, loss = 0.00990323\n",
            "Iteration 557, loss = 0.00984413\n",
            "Iteration 558, loss = 0.00986275\n",
            "Iteration 559, loss = 0.00983282\n",
            "Iteration 560, loss = 0.00979543\n",
            "Iteration 561, loss = 0.00983456\n",
            "Iteration 562, loss = 0.00984104\n",
            "Iteration 563, loss = 0.00974745\n",
            "Iteration 564, loss = 0.00968437\n",
            "Iteration 565, loss = 0.00980478\n",
            "Iteration 566, loss = 0.00967587\n",
            "Iteration 567, loss = 0.00978551\n",
            "Iteration 568, loss = 0.00970034\n",
            "Iteration 569, loss = 0.00967553\n",
            "Iteration 570, loss = 0.00960679\n",
            "Iteration 571, loss = 0.00958547\n",
            "Iteration 572, loss = 0.00956094\n",
            "Iteration 573, loss = 0.00960763\n",
            "Iteration 574, loss = 0.00953573\n",
            "Iteration 575, loss = 0.00954657\n",
            "Iteration 576, loss = 0.00950439\n",
            "Iteration 577, loss = 0.00954819\n",
            "Iteration 578, loss = 0.00945574\n",
            "Iteration 579, loss = 0.00944501\n",
            "Iteration 580, loss = 0.00939139\n",
            "Iteration 581, loss = 0.00937651\n",
            "Iteration 582, loss = 0.00937264\n",
            "Iteration 583, loss = 0.00932595\n",
            "Iteration 584, loss = 0.00931620\n",
            "Iteration 585, loss = 0.00946354\n",
            "Iteration 586, loss = 0.00934019\n",
            "Iteration 587, loss = 0.00935500\n",
            "Iteration 588, loss = 0.00928094\n",
            "Iteration 589, loss = 0.00931517\n",
            "Iteration 590, loss = 0.00946299\n",
            "Iteration 591, loss = 0.00917800\n",
            "Iteration 592, loss = 0.00936817\n",
            "Iteration 593, loss = 0.00914394\n",
            "Iteration 594, loss = 0.00911556\n",
            "Iteration 595, loss = 0.00911499\n",
            "Iteration 596, loss = 0.00916734\n",
            "Iteration 597, loss = 0.00908318\n",
            "Iteration 598, loss = 0.00905999\n",
            "Iteration 599, loss = 0.00906581\n",
            "Iteration 600, loss = 0.00902752\n",
            "Iteration 601, loss = 0.00912314\n",
            "Iteration 602, loss = 0.00903869\n",
            "Iteration 603, loss = 0.00897920\n",
            "Iteration 604, loss = 0.00896899\n",
            "Iteration 605, loss = 0.00897670\n",
            "Iteration 606, loss = 0.00895588\n",
            "Iteration 607, loss = 0.00901818\n",
            "Iteration 608, loss = 0.00893504\n",
            "Iteration 609, loss = 0.00885543\n",
            "Iteration 610, loss = 0.00885516\n",
            "Iteration 611, loss = 0.00885204\n",
            "Iteration 612, loss = 0.00878565\n",
            "Iteration 613, loss = 0.00883080\n",
            "Iteration 614, loss = 0.00882229\n",
            "Iteration 615, loss = 0.00893320\n",
            "Iteration 616, loss = 0.00879977\n",
            "Iteration 617, loss = 0.00885627\n",
            "Iteration 618, loss = 0.00869711\n",
            "Iteration 619, loss = 0.00870755\n",
            "Iteration 620, loss = 0.00868467\n",
            "Iteration 621, loss = 0.00866321\n",
            "Iteration 622, loss = 0.00865255\n",
            "Iteration 623, loss = 0.00869191\n",
            "Iteration 624, loss = 0.00856521\n",
            "Iteration 625, loss = 0.00858627\n",
            "Iteration 626, loss = 0.00863259\n",
            "Iteration 627, loss = 0.00856915\n",
            "Iteration 628, loss = 0.00861656\n",
            "Iteration 629, loss = 0.00860125\n",
            "Iteration 630, loss = 0.00854386\n",
            "Iteration 631, loss = 0.00855717\n",
            "Iteration 632, loss = 0.00845325\n",
            "Iteration 633, loss = 0.00850065\n",
            "Iteration 634, loss = 0.00843783\n",
            "Iteration 635, loss = 0.00849434\n",
            "Iteration 636, loss = 0.00839547\n",
            "Iteration 637, loss = 0.00849057\n",
            "Iteration 638, loss = 0.00837913\n",
            "Iteration 639, loss = 0.00839721\n",
            "Iteration 640, loss = 0.00837704\n",
            "Iteration 641, loss = 0.00835490\n",
            "Iteration 642, loss = 0.00834298\n",
            "Iteration 643, loss = 0.00824529\n",
            "Iteration 644, loss = 0.00831177\n",
            "Iteration 645, loss = 0.00834366\n",
            "Iteration 646, loss = 0.00834292\n",
            "Iteration 647, loss = 0.00822883\n",
            "Iteration 648, loss = 0.00830366\n",
            "Iteration 649, loss = 0.00825160\n",
            "Iteration 650, loss = 0.00816866\n",
            "Iteration 651, loss = 0.00817853\n",
            "Iteration 652, loss = 0.00813867\n",
            "Iteration 653, loss = 0.00815736\n",
            "Iteration 654, loss = 0.00819458\n",
            "Iteration 655, loss = 0.00819978\n",
            "Iteration 656, loss = 0.00811884\n",
            "Iteration 657, loss = 0.00810645\n",
            "Iteration 658, loss = 0.00807316\n",
            "Iteration 659, loss = 0.00822491\n",
            "Iteration 660, loss = 0.00802865\n",
            "Iteration 661, loss = 0.00807497\n",
            "Iteration 662, loss = 0.00806217\n",
            "Iteration 663, loss = 0.00805030\n",
            "Iteration 664, loss = 0.00800785\n",
            "Iteration 665, loss = 0.00802713\n",
            "Iteration 666, loss = 0.00799593\n",
            "Iteration 667, loss = 0.00791372\n",
            "Iteration 668, loss = 0.00790381\n",
            "Iteration 669, loss = 0.00797139\n",
            "Iteration 670, loss = 0.00787313\n",
            "Iteration 671, loss = 0.00797517\n",
            "Iteration 672, loss = 0.00792547\n",
            "Iteration 673, loss = 0.00783587\n",
            "Iteration 674, loss = 0.00781682\n",
            "Iteration 675, loss = 0.00781873\n",
            "Iteration 676, loss = 0.00792121\n",
            "Iteration 677, loss = 0.00779263\n",
            "Iteration 678, loss = 0.00778203\n",
            "Iteration 679, loss = 0.00787619\n",
            "Iteration 680, loss = 0.00786216\n",
            "Iteration 681, loss = 0.00771399\n",
            "Iteration 682, loss = 0.00769408\n",
            "Iteration 683, loss = 0.00773869\n",
            "Iteration 684, loss = 0.00771118\n",
            "Iteration 685, loss = 0.00770614\n",
            "Iteration 686, loss = 0.00765186\n",
            "Iteration 687, loss = 0.00764903\n",
            "Iteration 688, loss = 0.00765366\n",
            "Iteration 689, loss = 0.00761622\n",
            "Iteration 690, loss = 0.00770890\n",
            "Iteration 691, loss = 0.00757827\n",
            "Iteration 692, loss = 0.00757682\n",
            "Iteration 693, loss = 0.00759708\n",
            "Iteration 694, loss = 0.00754241\n",
            "Iteration 695, loss = 0.00758475\n",
            "Iteration 696, loss = 0.00756821\n",
            "Iteration 697, loss = 0.00756497\n",
            "Iteration 698, loss = 0.00758404\n",
            "Iteration 699, loss = 0.00764079\n",
            "Iteration 700, loss = 0.00746897\n",
            "Iteration 701, loss = 0.00745140\n",
            "Iteration 702, loss = 0.00742668\n",
            "Iteration 703, loss = 0.00746007\n",
            "Iteration 704, loss = 0.00742973\n",
            "Iteration 705, loss = 0.00739402\n",
            "Iteration 706, loss = 0.00739419\n",
            "Iteration 707, loss = 0.00750717\n",
            "Iteration 708, loss = 0.00739569\n",
            "Iteration 709, loss = 0.00736875\n",
            "Iteration 710, loss = 0.00737987\n",
            "Iteration 711, loss = 0.00736391\n",
            "Iteration 712, loss = 0.00732101\n",
            "Iteration 713, loss = 0.00729911\n",
            "Iteration 714, loss = 0.00727574\n",
            "Iteration 715, loss = 0.00731575\n",
            "Iteration 716, loss = 0.00735704\n",
            "Iteration 717, loss = 0.00726085\n",
            "Iteration 718, loss = 0.00723809\n",
            "Iteration 719, loss = 0.00735938\n",
            "Iteration 720, loss = 0.00718716\n",
            "Iteration 721, loss = 0.00719632\n",
            "Iteration 722, loss = 0.00723094\n",
            "Iteration 723, loss = 0.00718973\n",
            "Iteration 724, loss = 0.00718073\n",
            "Iteration 725, loss = 0.00713307\n",
            "Iteration 726, loss = 0.00713146\n",
            "Iteration 727, loss = 0.00714521\n",
            "Iteration 728, loss = 0.00711727\n",
            "Iteration 729, loss = 0.00711862\n",
            "Iteration 730, loss = 0.00707971\n",
            "Iteration 731, loss = 0.00713062\n",
            "Iteration 732, loss = 0.00716552\n",
            "Iteration 733, loss = 0.00702775\n",
            "Iteration 734, loss = 0.00704304\n",
            "Iteration 735, loss = 0.00712872\n",
            "Iteration 736, loss = 0.00710765\n",
            "Iteration 737, loss = 0.00700067\n",
            "Iteration 738, loss = 0.00701395\n",
            "Iteration 739, loss = 0.00703760\n",
            "Iteration 740, loss = 0.00696220\n",
            "Iteration 741, loss = 0.00691688\n",
            "Iteration 742, loss = 0.00693783\n",
            "Iteration 743, loss = 0.00699343\n",
            "Iteration 744, loss = 0.00690665\n",
            "Iteration 745, loss = 0.00692653\n",
            "Iteration 746, loss = 0.00687366\n",
            "Iteration 747, loss = 0.00691584\n",
            "Iteration 748, loss = 0.00689507\n",
            "Iteration 749, loss = 0.00684161\n",
            "Iteration 750, loss = 0.00691146\n",
            "Iteration 751, loss = 0.00684532\n",
            "Iteration 752, loss = 0.00678361\n",
            "Iteration 753, loss = 0.00692312\n",
            "Iteration 754, loss = 0.00686507\n",
            "Iteration 755, loss = 0.00682494\n",
            "Iteration 756, loss = 0.00683377\n",
            "Iteration 757, loss = 0.00682635\n",
            "Iteration 758, loss = 0.00679330\n",
            "Iteration 759, loss = 0.00675862\n",
            "Iteration 760, loss = 0.00672647\n",
            "Iteration 761, loss = 0.00670511\n",
            "Iteration 762, loss = 0.00679540\n",
            "Iteration 763, loss = 0.00668660\n",
            "Iteration 764, loss = 0.00673779\n",
            "Iteration 765, loss = 0.00674691\n",
            "Iteration 766, loss = 0.00671483\n",
            "Iteration 767, loss = 0.00659694\n",
            "Iteration 768, loss = 0.00664535\n",
            "Iteration 769, loss = 0.00660171\n",
            "Iteration 770, loss = 0.00662575\n",
            "Iteration 771, loss = 0.00657006\n",
            "Iteration 772, loss = 0.00658142\n",
            "Iteration 773, loss = 0.00659528\n",
            "Iteration 774, loss = 0.00655130\n",
            "Iteration 775, loss = 0.00657674\n",
            "Iteration 776, loss = 0.00659376\n",
            "Iteration 777, loss = 0.00648756\n",
            "Iteration 778, loss = 0.00654097\n",
            "Iteration 779, loss = 0.00644943\n",
            "Iteration 780, loss = 0.00658332\n",
            "Iteration 781, loss = 0.00655255\n",
            "Iteration 782, loss = 0.00655543\n",
            "Iteration 783, loss = 0.00648749\n",
            "Iteration 784, loss = 0.00647244\n",
            "Iteration 785, loss = 0.00642692\n",
            "Iteration 786, loss = 0.00645914\n",
            "Iteration 787, loss = 0.00642657\n",
            "Iteration 788, loss = 0.00639756\n",
            "Iteration 789, loss = 0.00643543\n",
            "Iteration 790, loss = 0.00634561\n",
            "Iteration 791, loss = 0.00639555\n",
            "Iteration 792, loss = 0.00634622\n",
            "Iteration 793, loss = 0.00635308\n",
            "Iteration 794, loss = 0.00634272\n",
            "Iteration 795, loss = 0.00633197\n",
            "Iteration 796, loss = 0.00634680\n",
            "Iteration 797, loss = 0.00630880\n",
            "Iteration 798, loss = 0.00633946\n",
            "Iteration 799, loss = 0.00631901\n",
            "Iteration 800, loss = 0.00631881\n",
            "Iteration 801, loss = 0.00629013\n",
            "Iteration 802, loss = 0.00630027\n",
            "Iteration 803, loss = 0.00633445\n",
            "Iteration 804, loss = 0.00623677\n",
            "Iteration 805, loss = 0.00626978\n",
            "Iteration 806, loss = 0.00630746\n",
            "Iteration 807, loss = 0.00622293\n",
            "Iteration 808, loss = 0.00621686\n",
            "Iteration 809, loss = 0.00615129\n",
            "Iteration 810, loss = 0.00615470\n",
            "Iteration 811, loss = 0.00614855\n",
            "Iteration 812, loss = 0.00619977\n",
            "Iteration 813, loss = 0.00611094\n",
            "Iteration 814, loss = 0.00612850\n",
            "Iteration 815, loss = 0.00615250\n",
            "Iteration 816, loss = 0.00608394\n",
            "Iteration 817, loss = 0.00609864\n",
            "Iteration 818, loss = 0.00612641\n",
            "Iteration 819, loss = 0.00610844\n",
            "Iteration 820, loss = 0.00614034\n",
            "Iteration 821, loss = 0.00612040\n",
            "Iteration 822, loss = 0.00607718\n",
            "Iteration 823, loss = 0.00605579\n",
            "Iteration 824, loss = 0.00605560\n",
            "Iteration 825, loss = 0.00599569\n",
            "Iteration 826, loss = 0.00609096\n",
            "Iteration 827, loss = 0.00601181\n",
            "Iteration 828, loss = 0.00601949\n",
            "Iteration 829, loss = 0.00602192\n",
            "Iteration 830, loss = 0.00601330\n",
            "Iteration 831, loss = 0.00596277\n",
            "Iteration 832, loss = 0.00597071\n",
            "Iteration 833, loss = 0.00594168\n",
            "Iteration 834, loss = 0.00594050\n",
            "Iteration 835, loss = 0.00603206\n",
            "Iteration 836, loss = 0.00588317\n",
            "Iteration 837, loss = 0.00590877\n",
            "Iteration 838, loss = 0.00597445\n",
            "Iteration 839, loss = 0.00595824\n",
            "Iteration 840, loss = 0.00591598\n",
            "Iteration 841, loss = 0.00591395\n",
            "Iteration 842, loss = 0.00587946\n",
            "Iteration 843, loss = 0.00585233\n",
            "Iteration 844, loss = 0.00580348\n",
            "Iteration 845, loss = 0.00585449\n",
            "Iteration 846, loss = 0.00581447\n",
            "Iteration 847, loss = 0.00579197\n",
            "Iteration 848, loss = 0.00581445\n",
            "Iteration 849, loss = 0.00582299\n",
            "Iteration 850, loss = 0.00586129\n",
            "Iteration 851, loss = 0.00579496\n",
            "Iteration 852, loss = 0.00580330\n",
            "Iteration 853, loss = 0.00579322\n",
            "Iteration 854, loss = 0.00576749\n",
            "Iteration 855, loss = 0.00574071\n",
            "Iteration 856, loss = 0.00575470\n",
            "Iteration 857, loss = 0.00575568\n",
            "Iteration 858, loss = 0.00576179\n",
            "Iteration 859, loss = 0.00567459\n",
            "Iteration 860, loss = 0.00566950\n",
            "Iteration 861, loss = 0.00577767\n",
            "Iteration 862, loss = 0.00567822\n",
            "Iteration 863, loss = 0.00566060\n",
            "Iteration 864, loss = 0.00566444\n",
            "Iteration 865, loss = 0.00567781\n",
            "Iteration 866, loss = 0.00566127\n",
            "Iteration 867, loss = 0.00563242\n",
            "Iteration 868, loss = 0.00559663\n",
            "Iteration 869, loss = 0.00558348\n",
            "Iteration 870, loss = 0.00562101\n",
            "Iteration 871, loss = 0.00562658\n",
            "Iteration 872, loss = 0.00559847\n",
            "Iteration 873, loss = 0.00555070\n",
            "Iteration 874, loss = 0.00560100\n",
            "Iteration 875, loss = 0.00552890\n",
            "Iteration 876, loss = 0.00561280\n",
            "Iteration 877, loss = 0.00558261\n",
            "Iteration 878, loss = 0.00554279\n",
            "Iteration 879, loss = 0.00551927\n",
            "Iteration 880, loss = 0.00561004\n",
            "Iteration 881, loss = 0.00551981\n",
            "Iteration 882, loss = 0.00555762\n",
            "Iteration 883, loss = 0.00548034\n",
            "Iteration 884, loss = 0.00548654\n",
            "Iteration 885, loss = 0.00554692\n",
            "Iteration 886, loss = 0.00546151\n",
            "Iteration 887, loss = 0.00544214\n",
            "Iteration 888, loss = 0.00543465\n",
            "Iteration 889, loss = 0.00547165\n",
            "Iteration 890, loss = 0.00544648\n",
            "Iteration 891, loss = 0.00551644\n",
            "Iteration 892, loss = 0.00551388\n",
            "Iteration 893, loss = 0.00546157\n",
            "Iteration 894, loss = 0.00542388\n",
            "Iteration 895, loss = 0.00542316\n",
            "Iteration 896, loss = 0.00539041\n",
            "Iteration 897, loss = 0.00545172\n",
            "Iteration 898, loss = 0.00538515\n",
            "Iteration 899, loss = 0.00539958\n",
            "Iteration 900, loss = 0.00533893\n",
            "Iteration 901, loss = 0.00533835\n",
            "Iteration 902, loss = 0.00536035\n",
            "Iteration 903, loss = 0.00539126\n",
            "Iteration 904, loss = 0.00534198\n",
            "Iteration 905, loss = 0.00526767\n",
            "Iteration 906, loss = 0.00526700\n",
            "Iteration 907, loss = 0.00532356\n",
            "Iteration 908, loss = 0.00528903\n",
            "Iteration 909, loss = 0.00526696\n",
            "Iteration 910, loss = 0.00528698\n",
            "Iteration 911, loss = 0.00530098\n",
            "Iteration 912, loss = 0.00533126\n",
            "Iteration 913, loss = 0.00528123\n",
            "Iteration 914, loss = 0.00524321\n",
            "Iteration 915, loss = 0.00524276\n",
            "Iteration 916, loss = 0.00520962\n",
            "Iteration 917, loss = 0.00530335\n",
            "Iteration 918, loss = 0.00523013\n",
            "Iteration 919, loss = 0.00513625\n",
            "Iteration 920, loss = 0.00516213\n",
            "Iteration 921, loss = 0.00530329\n",
            "Iteration 922, loss = 0.00527708\n",
            "Iteration 923, loss = 0.00514337\n",
            "Iteration 924, loss = 0.00514811\n",
            "Iteration 925, loss = 0.00517791\n",
            "Iteration 926, loss = 0.00513434\n",
            "Iteration 927, loss = 0.00507999\n",
            "Iteration 928, loss = 0.00525207\n",
            "Iteration 929, loss = 0.00507919\n",
            "Iteration 930, loss = 0.00514295\n",
            "Iteration 931, loss = 0.00519141\n",
            "Iteration 932, loss = 0.00508952\n",
            "Iteration 933, loss = 0.00513744\n",
            "Iteration 934, loss = 0.00505652\n",
            "Iteration 935, loss = 0.00507359\n",
            "Iteration 936, loss = 0.00506113\n",
            "Iteration 937, loss = 0.00504787\n",
            "Iteration 938, loss = 0.00506405\n",
            "Iteration 939, loss = 0.00504491\n",
            "Iteration 940, loss = 0.00501944\n",
            "Iteration 941, loss = 0.00502504\n",
            "Iteration 942, loss = 0.00502358\n",
            "Iteration 943, loss = 0.00500445\n",
            "Iteration 944, loss = 0.00499856\n",
            "Iteration 945, loss = 0.00499116\n",
            "Iteration 946, loss = 0.00501899\n",
            "Iteration 947, loss = 0.00500442\n",
            "Iteration 948, loss = 0.00495100\n",
            "Iteration 949, loss = 0.00493248\n",
            "Iteration 950, loss = 0.00493183\n",
            "Iteration 951, loss = 0.00494803\n",
            "Iteration 952, loss = 0.00499003\n",
            "Iteration 953, loss = 0.00498543\n",
            "Iteration 954, loss = 0.00490014\n",
            "Iteration 955, loss = 0.00491358\n",
            "Iteration 956, loss = 0.00490033\n",
            "Iteration 957, loss = 0.00494566\n",
            "Iteration 958, loss = 0.00488374\n",
            "Iteration 959, loss = 0.00487870\n",
            "Iteration 960, loss = 0.00484527\n",
            "Iteration 961, loss = 0.00499317\n",
            "Iteration 962, loss = 0.00484226\n",
            "Iteration 963, loss = 0.00502439\n",
            "Iteration 964, loss = 0.00485082\n",
            "Iteration 965, loss = 0.00494552\n",
            "Iteration 966, loss = 0.00486219\n",
            "Iteration 967, loss = 0.00479047\n",
            "Iteration 968, loss = 0.00484215\n",
            "Iteration 969, loss = 0.00487276\n",
            "Iteration 970, loss = 0.00483745\n",
            "Iteration 971, loss = 0.00478920\n",
            "Iteration 972, loss = 0.00481247\n",
            "Iteration 973, loss = 0.00478582\n",
            "Iteration 974, loss = 0.00471852\n",
            "Iteration 975, loss = 0.00481872\n",
            "Iteration 976, loss = 0.00479294\n",
            "Iteration 977, loss = 0.00469497\n",
            "Iteration 978, loss = 0.00482354\n",
            "Iteration 979, loss = 0.00475977\n",
            "Iteration 980, loss = 0.00474540\n",
            "Iteration 981, loss = 0.00475560\n",
            "Iteration 982, loss = 0.00469574\n",
            "Iteration 983, loss = 0.00468672\n",
            "Iteration 984, loss = 0.00467811\n",
            "Iteration 985, loss = 0.00482311\n",
            "Iteration 986, loss = 0.00470895\n",
            "Iteration 987, loss = 0.00467998\n",
            "Iteration 988, loss = 0.00472820\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=100, learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=1e-05, validation_fraction=0.1, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB9D8pquPQJB"
      },
      "source": [
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO67DHGNQxcp"
      },
      "source": [
        "Resultado da previsão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7z0-RGIQZbQ",
        "outputId": "72053328-ccce-4a27-eb5c-2e93210bd3e0"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "print(precisao)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2aBOrKXRRpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44fc9b48-7505-4138-efd3-109b0d1199e0"
      },
      "source": [
        "matriz = confusion_matrix(classe_teste, previsoes)\r\n",
        "matriz"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[435,   1],\n",
              "       [  1,  63]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}