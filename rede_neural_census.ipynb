{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rede_neural_census.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/RumxoDsWSkflOzSpyrWt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z20poExQzaeJ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "base = pd.read_csv('census.csv')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REAkvnfQUMuV"
      },
      "source": [
        "Transfomação de variáveis categóricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv230P_06ATS"
      },
      "source": [
        "previsores = base.iloc[:, 0:14].values\r\n",
        "classe = base.iloc[:, 14].values"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMd3j3mq76P5"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hewv_drf3u1w"
      },
      "source": [
        "labelencoder_previsores = LabelEncoder()\r\n",
        "previsores[:, 1] = labelencoder_previsores.fit_transform(previsores[:,1])\r\n",
        "previsores[:, 3] = labelencoder_previsores.fit_transform(previsores[:,3])\r\n",
        "previsores[:, 5] = labelencoder_previsores.fit_transform(previsores[:,5])\r\n",
        "previsores[:, 6] = labelencoder_previsores.fit_transform(previsores[:,6])\r\n",
        "previsores[:, 7] = labelencoder_previsores.fit_transform(previsores[:,7])\r\n",
        "previsores[:, 8] = labelencoder_previsores.fit_transform(previsores[:,8])\r\n",
        "previsores[:, 9] = labelencoder_previsores.fit_transform(previsores[:,9])\r\n",
        "previsores[:, 13] = labelencoder_previsores.fit_transform(previsores[:,13])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9IRFqJ59P_o"
      },
      "source": [
        "onehotencorder = ColumnTransformer(transformers=[(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])],remainder='passthrough')\n",
        "previsores = onehotencorder.fit_transform(previsores).toarray()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12cTsvUaTjYV"
      },
      "source": [
        "labelencorder_classe = LabelEncoder()\n",
        "classe = labelencorder_classe.fit_transform(classe)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TJCZzvwUGZQ"
      },
      "source": [
        "Escalonamento dos valoers numéricos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3XPVUp6TwDX"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\r\n",
        "scaler = StandardScaler()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxb6VFhPUsZ-"
      },
      "source": [
        "previsores = scaler.fit_transform(previsores)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C130M6zFUxBf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste =  train_test_split(previsores, classe, test_size=0.15, random_state=0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZy6HIS9UVQJ"
      },
      "source": [
        "Treinamento tensor flow\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SiTJqMKniD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8518ec-99f9-4703-a1d7-62d736539ff1"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\r\n",
        "classificador = MLPClassifier(verbose=True, max_iter=1000, tol=0.000010)\r\n",
        "classificador.fit(previsores_treinamento, classe_treinamento)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.38044674\n",
            "Iteration 2, loss = 0.32651168\n",
            "Iteration 3, loss = 0.31621319\n",
            "Iteration 4, loss = 0.31044233\n",
            "Iteration 5, loss = 0.30630636\n",
            "Iteration 6, loss = 0.30250458\n",
            "Iteration 7, loss = 0.30001322\n",
            "Iteration 8, loss = 0.29701525\n",
            "Iteration 9, loss = 0.29494038\n",
            "Iteration 10, loss = 0.29326693\n",
            "Iteration 11, loss = 0.29099984\n",
            "Iteration 12, loss = 0.29037263\n",
            "Iteration 13, loss = 0.28707287\n",
            "Iteration 14, loss = 0.28632038\n",
            "Iteration 15, loss = 0.28467447\n",
            "Iteration 16, loss = 0.28360786\n",
            "Iteration 17, loss = 0.28207526\n",
            "Iteration 18, loss = 0.28086725\n",
            "Iteration 19, loss = 0.27981511\n",
            "Iteration 20, loss = 0.27813679\n",
            "Iteration 21, loss = 0.27652957\n",
            "Iteration 22, loss = 0.27627031\n",
            "Iteration 23, loss = 0.27469060\n",
            "Iteration 24, loss = 0.27378048\n",
            "Iteration 25, loss = 0.27341736\n",
            "Iteration 26, loss = 0.27173806\n",
            "Iteration 27, loss = 0.27066445\n",
            "Iteration 28, loss = 0.27095509\n",
            "Iteration 29, loss = 0.26909547\n",
            "Iteration 30, loss = 0.26891002\n",
            "Iteration 31, loss = 0.26741528\n",
            "Iteration 32, loss = 0.26646112\n",
            "Iteration 33, loss = 0.26717772\n",
            "Iteration 34, loss = 0.26506297\n",
            "Iteration 35, loss = 0.26418171\n",
            "Iteration 36, loss = 0.26388066\n",
            "Iteration 37, loss = 0.26296542\n",
            "Iteration 38, loss = 0.26225841\n",
            "Iteration 39, loss = 0.26227321\n",
            "Iteration 40, loss = 0.26156317\n",
            "Iteration 41, loss = 0.26026544\n",
            "Iteration 42, loss = 0.25969622\n",
            "Iteration 43, loss = 0.25905496\n",
            "Iteration 44, loss = 0.25896767\n",
            "Iteration 45, loss = 0.25727662\n",
            "Iteration 46, loss = 0.25736498\n",
            "Iteration 47, loss = 0.25679810\n",
            "Iteration 48, loss = 0.25650868\n",
            "Iteration 49, loss = 0.25532754\n",
            "Iteration 50, loss = 0.25509879\n",
            "Iteration 51, loss = 0.25418884\n",
            "Iteration 52, loss = 0.25511535\n",
            "Iteration 53, loss = 0.25259376\n",
            "Iteration 54, loss = 0.25314509\n",
            "Iteration 55, loss = 0.25271003\n",
            "Iteration 56, loss = 0.25169733\n",
            "Iteration 57, loss = 0.25156142\n",
            "Iteration 58, loss = 0.25014271\n",
            "Iteration 59, loss = 0.25147159\n",
            "Iteration 60, loss = 0.24930669\n",
            "Iteration 61, loss = 0.24962555\n",
            "Iteration 62, loss = 0.24884652\n",
            "Iteration 63, loss = 0.24853900\n",
            "Iteration 64, loss = 0.24837462\n",
            "Iteration 65, loss = 0.24755583\n",
            "Iteration 66, loss = 0.24691938\n",
            "Iteration 67, loss = 0.24679883\n",
            "Iteration 68, loss = 0.24563431\n",
            "Iteration 69, loss = 0.24522357\n",
            "Iteration 70, loss = 0.24458373\n",
            "Iteration 71, loss = 0.24568359\n",
            "Iteration 72, loss = 0.24447227\n",
            "Iteration 73, loss = 0.24368252\n",
            "Iteration 74, loss = 0.24286804\n",
            "Iteration 75, loss = 0.24364777\n",
            "Iteration 76, loss = 0.24207205\n",
            "Iteration 77, loss = 0.24223840\n",
            "Iteration 78, loss = 0.24252804\n",
            "Iteration 79, loss = 0.24269454\n",
            "Iteration 80, loss = 0.24153026\n",
            "Iteration 81, loss = 0.24100434\n",
            "Iteration 82, loss = 0.24018312\n",
            "Iteration 83, loss = 0.24136615\n",
            "Iteration 84, loss = 0.23927829\n",
            "Iteration 85, loss = 0.23896120\n",
            "Iteration 86, loss = 0.23885841\n",
            "Iteration 87, loss = 0.23904628\n",
            "Iteration 88, loss = 0.23900214\n",
            "Iteration 89, loss = 0.23812294\n",
            "Iteration 90, loss = 0.23764676\n",
            "Iteration 91, loss = 0.23656384\n",
            "Iteration 92, loss = 0.23620353\n",
            "Iteration 93, loss = 0.23661422\n",
            "Iteration 94, loss = 0.23593360\n",
            "Iteration 95, loss = 0.23604953\n",
            "Iteration 96, loss = 0.23596446\n",
            "Iteration 97, loss = 0.23680029\n",
            "Iteration 98, loss = 0.23578937\n",
            "Iteration 99, loss = 0.23408597\n",
            "Iteration 100, loss = 0.23376845\n",
            "Iteration 101, loss = 0.23398072\n",
            "Iteration 102, loss = 0.23366006\n",
            "Iteration 103, loss = 0.23415464\n",
            "Iteration 104, loss = 0.23290430\n",
            "Iteration 105, loss = 0.23206429\n",
            "Iteration 106, loss = 0.23162065\n",
            "Iteration 107, loss = 0.23260088\n",
            "Iteration 108, loss = 0.23215740\n",
            "Iteration 109, loss = 0.23188073\n",
            "Iteration 110, loss = 0.23156476\n",
            "Iteration 111, loss = 0.22984810\n",
            "Iteration 112, loss = 0.23124097\n",
            "Iteration 113, loss = 0.22918104\n",
            "Iteration 114, loss = 0.23077365\n",
            "Iteration 115, loss = 0.22899338\n",
            "Iteration 116, loss = 0.22837920\n",
            "Iteration 117, loss = 0.22793142\n",
            "Iteration 118, loss = 0.22893524\n",
            "Iteration 119, loss = 0.22824281\n",
            "Iteration 120, loss = 0.22947050\n",
            "Iteration 121, loss = 0.22663838\n",
            "Iteration 122, loss = 0.22686352\n",
            "Iteration 123, loss = 0.22680251\n",
            "Iteration 124, loss = 0.22521885\n",
            "Iteration 125, loss = 0.22682896\n",
            "Iteration 126, loss = 0.22768214\n",
            "Iteration 127, loss = 0.22691495\n",
            "Iteration 128, loss = 0.22517757\n",
            "Iteration 129, loss = 0.22550173\n",
            "Iteration 130, loss = 0.22478366\n",
            "Iteration 131, loss = 0.22527846\n",
            "Iteration 132, loss = 0.22402204\n",
            "Iteration 133, loss = 0.22502867\n",
            "Iteration 134, loss = 0.22345398\n",
            "Iteration 135, loss = 0.22454493\n",
            "Iteration 136, loss = 0.22386527\n",
            "Iteration 137, loss = 0.22412902\n",
            "Iteration 138, loss = 0.22214852\n",
            "Iteration 139, loss = 0.22262802\n",
            "Iteration 140, loss = 0.22310694\n",
            "Iteration 141, loss = 0.22196204\n",
            "Iteration 142, loss = 0.22163828\n",
            "Iteration 143, loss = 0.22171456\n",
            "Iteration 144, loss = 0.22183969\n",
            "Iteration 145, loss = 0.22139694\n",
            "Iteration 146, loss = 0.22065411\n",
            "Iteration 147, loss = 0.21972175\n",
            "Iteration 148, loss = 0.22060721\n",
            "Iteration 149, loss = 0.22009942\n",
            "Iteration 150, loss = 0.21858559\n",
            "Iteration 151, loss = 0.22042787\n",
            "Iteration 152, loss = 0.21945306\n",
            "Iteration 153, loss = 0.21864390\n",
            "Iteration 154, loss = 0.21927213\n",
            "Iteration 155, loss = 0.21881856\n",
            "Iteration 156, loss = 0.21829293\n",
            "Iteration 157, loss = 0.21872381\n",
            "Iteration 158, loss = 0.21805195\n",
            "Iteration 159, loss = 0.21945048\n",
            "Iteration 160, loss = 0.21728717\n",
            "Iteration 161, loss = 0.21630912\n",
            "Iteration 162, loss = 0.21687863\n",
            "Iteration 163, loss = 0.21750873\n",
            "Iteration 164, loss = 0.21682303\n",
            "Iteration 165, loss = 0.21715943\n",
            "Iteration 166, loss = 0.21595402\n",
            "Iteration 167, loss = 0.21527985\n",
            "Iteration 168, loss = 0.21584856\n",
            "Iteration 169, loss = 0.21575898\n",
            "Iteration 170, loss = 0.21469372\n",
            "Iteration 171, loss = 0.21486796\n",
            "Iteration 172, loss = 0.21457148\n",
            "Iteration 173, loss = 0.21493497\n",
            "Iteration 174, loss = 0.21428032\n",
            "Iteration 175, loss = 0.21417349\n",
            "Iteration 176, loss = 0.21394566\n",
            "Iteration 177, loss = 0.21564081\n",
            "Iteration 178, loss = 0.21292193\n",
            "Iteration 179, loss = 0.21342755\n",
            "Iteration 180, loss = 0.21275159\n",
            "Iteration 181, loss = 0.21296267\n",
            "Iteration 182, loss = 0.21329539\n",
            "Iteration 183, loss = 0.21279423\n",
            "Iteration 184, loss = 0.21220222\n",
            "Iteration 185, loss = 0.21199066\n",
            "Iteration 186, loss = 0.21084594\n",
            "Iteration 187, loss = 0.21183926\n",
            "Iteration 188, loss = 0.21154257\n",
            "Iteration 189, loss = 0.21146647\n",
            "Iteration 190, loss = 0.21122091\n",
            "Iteration 191, loss = 0.21094584\n",
            "Iteration 192, loss = 0.21014163\n",
            "Iteration 193, loss = 0.21110244\n",
            "Iteration 194, loss = 0.21132397\n",
            "Iteration 195, loss = 0.21044920\n",
            "Iteration 196, loss = 0.20947934\n",
            "Iteration 197, loss = 0.20959085\n",
            "Iteration 198, loss = 0.20982731\n",
            "Iteration 199, loss = 0.21068560\n",
            "Iteration 200, loss = 0.20988947\n",
            "Iteration 201, loss = 0.21092295\n",
            "Iteration 202, loss = 0.20891239\n",
            "Iteration 203, loss = 0.20795255\n",
            "Iteration 204, loss = 0.20847515\n",
            "Iteration 205, loss = 0.20801098\n",
            "Iteration 206, loss = 0.20723821\n",
            "Iteration 207, loss = 0.20921813\n",
            "Iteration 208, loss = 0.20821336\n",
            "Iteration 209, loss = 0.20845767\n",
            "Iteration 210, loss = 0.20806047\n",
            "Iteration 211, loss = 0.20706856\n",
            "Iteration 212, loss = 0.20777437\n",
            "Iteration 213, loss = 0.20771165\n",
            "Iteration 214, loss = 0.20719263\n",
            "Iteration 215, loss = 0.20691440\n",
            "Iteration 216, loss = 0.20542233\n",
            "Iteration 217, loss = 0.20574195\n",
            "Iteration 218, loss = 0.20776440\n",
            "Iteration 219, loss = 0.20661235\n",
            "Iteration 220, loss = 0.20556139\n",
            "Iteration 221, loss = 0.20625573\n",
            "Iteration 222, loss = 0.20514992\n",
            "Iteration 223, loss = 0.20492857\n",
            "Iteration 224, loss = 0.20491864\n",
            "Iteration 225, loss = 0.20496553\n",
            "Iteration 226, loss = 0.20600983\n",
            "Iteration 227, loss = 0.20565225\n",
            "Iteration 228, loss = 0.20432421\n",
            "Iteration 229, loss = 0.20475685\n",
            "Iteration 230, loss = 0.20622516\n",
            "Iteration 231, loss = 0.20437922\n",
            "Iteration 232, loss = 0.20430474\n",
            "Iteration 233, loss = 0.20404298\n",
            "Iteration 234, loss = 0.20451047\n",
            "Iteration 235, loss = 0.20374063\n",
            "Iteration 236, loss = 0.20452449\n",
            "Iteration 237, loss = 0.20283626\n",
            "Iteration 238, loss = 0.20286898\n",
            "Iteration 239, loss = 0.20380732\n",
            "Iteration 240, loss = 0.20326628\n",
            "Iteration 241, loss = 0.20374801\n",
            "Iteration 242, loss = 0.20226413\n",
            "Iteration 243, loss = 0.20231757\n",
            "Iteration 244, loss = 0.20290567\n",
            "Iteration 245, loss = 0.20295335\n",
            "Iteration 246, loss = 0.20239332\n",
            "Iteration 247, loss = 0.20114436\n",
            "Iteration 248, loss = 0.20185470\n",
            "Iteration 249, loss = 0.20163472\n",
            "Iteration 250, loss = 0.20185699\n",
            "Iteration 251, loss = 0.20220935\n",
            "Iteration 252, loss = 0.20213100\n",
            "Iteration 253, loss = 0.20117830\n",
            "Iteration 254, loss = 0.20048933\n",
            "Iteration 255, loss = 0.19975720\n",
            "Iteration 256, loss = 0.20061727\n",
            "Iteration 257, loss = 0.20163336\n",
            "Iteration 258, loss = 0.20012082\n",
            "Iteration 259, loss = 0.20040835\n",
            "Iteration 260, loss = 0.20042511\n",
            "Iteration 261, loss = 0.20086903\n",
            "Iteration 262, loss = 0.19963297\n",
            "Iteration 263, loss = 0.20156223\n",
            "Iteration 264, loss = 0.19917971\n",
            "Iteration 265, loss = 0.19985669\n",
            "Iteration 266, loss = 0.19881422\n",
            "Iteration 267, loss = 0.19872758\n",
            "Iteration 268, loss = 0.19818508\n",
            "Iteration 269, loss = 0.20046886\n",
            "Iteration 270, loss = 0.19945145\n",
            "Iteration 271, loss = 0.19996825\n",
            "Iteration 272, loss = 0.19902908\n",
            "Iteration 273, loss = 0.19901555\n",
            "Iteration 274, loss = 0.19811304\n",
            "Iteration 275, loss = 0.19824870\n",
            "Iteration 276, loss = 0.19830705\n",
            "Iteration 277, loss = 0.19809397\n",
            "Iteration 278, loss = 0.19744750\n",
            "Iteration 279, loss = 0.19821816\n",
            "Iteration 280, loss = 0.19680880\n",
            "Iteration 281, loss = 0.19885363\n",
            "Iteration 282, loss = 0.19791226\n",
            "Iteration 283, loss = 0.19778065\n",
            "Iteration 284, loss = 0.19720451\n",
            "Iteration 285, loss = 0.19749901\n",
            "Iteration 286, loss = 0.19652979\n",
            "Iteration 287, loss = 0.19720539\n",
            "Iteration 288, loss = 0.19768240\n",
            "Iteration 289, loss = 0.19780366\n",
            "Iteration 290, loss = 0.19741458\n",
            "Iteration 291, loss = 0.19730718\n",
            "Iteration 292, loss = 0.19745811\n",
            "Iteration 293, loss = 0.19749082\n",
            "Iteration 294, loss = 0.19630852\n",
            "Iteration 295, loss = 0.19551258\n",
            "Iteration 296, loss = 0.19666449\n",
            "Iteration 297, loss = 0.19590590\n",
            "Iteration 298, loss = 0.19652706\n",
            "Iteration 299, loss = 0.19616460\n",
            "Iteration 300, loss = 0.19621910\n",
            "Iteration 301, loss = 0.19529409\n",
            "Iteration 302, loss = 0.19554901\n",
            "Iteration 303, loss = 0.19572145\n",
            "Iteration 304, loss = 0.19616531\n",
            "Iteration 305, loss = 0.19488648\n",
            "Iteration 306, loss = 0.19512657\n",
            "Iteration 307, loss = 0.19455326\n",
            "Iteration 308, loss = 0.19578107\n",
            "Iteration 309, loss = 0.19535070\n",
            "Iteration 310, loss = 0.19429024\n",
            "Iteration 311, loss = 0.19460036\n",
            "Iteration 312, loss = 0.19397848\n",
            "Iteration 313, loss = 0.19341782\n",
            "Iteration 314, loss = 0.19447895\n",
            "Iteration 315, loss = 0.19350455\n",
            "Iteration 316, loss = 0.19493313\n",
            "Iteration 317, loss = 0.19432441\n",
            "Iteration 318, loss = 0.19612471\n",
            "Iteration 319, loss = 0.19440206\n",
            "Iteration 320, loss = 0.19330533\n",
            "Iteration 321, loss = 0.19313127\n",
            "Iteration 322, loss = 0.19363157\n",
            "Iteration 323, loss = 0.19230240\n",
            "Iteration 324, loss = 0.19290492\n",
            "Iteration 325, loss = 0.19298230\n",
            "Iteration 326, loss = 0.19341779\n",
            "Iteration 327, loss = 0.19320408\n",
            "Iteration 328, loss = 0.19342452\n",
            "Iteration 329, loss = 0.19158044\n",
            "Iteration 330, loss = 0.19265631\n",
            "Iteration 331, loss = 0.19230515\n",
            "Iteration 332, loss = 0.19241480\n",
            "Iteration 333, loss = 0.19269802\n",
            "Iteration 334, loss = 0.19308193\n",
            "Iteration 335, loss = 0.19304070\n",
            "Iteration 336, loss = 0.19226108\n",
            "Iteration 337, loss = 0.19200459\n",
            "Iteration 338, loss = 0.19214443\n",
            "Iteration 339, loss = 0.19011550\n",
            "Iteration 340, loss = 0.19143796\n",
            "Iteration 341, loss = 0.19169821\n",
            "Iteration 342, loss = 0.19125370\n",
            "Iteration 343, loss = 0.19390192\n",
            "Iteration 344, loss = 0.19101604\n",
            "Iteration 345, loss = 0.19237722\n",
            "Iteration 346, loss = 0.19095952\n",
            "Iteration 347, loss = 0.19173191\n",
            "Iteration 348, loss = 0.19215819\n",
            "Iteration 349, loss = 0.19155351\n",
            "Iteration 350, loss = 0.19090161\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=1e-05, validation_fraction=0.1, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK90iXOMUf2q"
      },
      "source": [
        "Resultado da previsão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3JFDrnGUgkq"
      },
      "source": [
        "previsoes = classificador.predict(previsores_teste)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG9frNDvUkSr",
        "outputId": "03570f80-900b-4f7a-ad42-e0eafcdb4dd0"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "precisao = accuracy_score(classe_teste, previsoes)\n",
        "print(precisao)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8316953316953317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bitwVS3DUpGT",
        "outputId": "d56dcc76-382e-4783-aa1b-595aa5d7f33d"
      },
      "source": [
        "matriz = confusion_matrix(classe_teste, previsoes)\n",
        "print(matriz)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3392  367]\n",
            " [ 455  670]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}